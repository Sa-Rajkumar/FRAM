# -*- coding: utf-8 -*-
"""GOOD_RUNS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KqdYlvD6KwktXhy_bs1_NHce_RaoM59T
"""

import pandas as pd

df1=pd.read_excel('Tags1.xlsx',sheet_name='G1')
df2=pd.read_excel('Tags1.xlsx',sheet_name='G2')
df3=pd.read_excel('Tags1.xlsx',sheet_name='G3')
df4=pd.read_excel('Tags1.xlsx',sheet_name='G4')
df5=pd.read_excel('Tags1.xlsx',sheet_name='G5')
df6=pd.read_excel('Tags1.xlsx',sheet_name='G6')
df7=pd.read_excel('Tags1.xlsx',sheet_name='G7')
df8=pd.read_excel('Tags1.xlsx',sheet_name='G8')
df9=pd.read_excel('Tags1.xlsx',sheet_name='G9')
df10=pd.read_excel('Tags1.xlsx',sheet_name='G10')

dfgd=pd.read_excel('GD_RUNS.xlsx',sheet_name='Sheet1')
dfgd

df=pd.read_excel('MIX_FAILURE.xlsx')
df

start_time = df['Start Time']
end_time = df['End Time']
G_number = df['Gasifier No.']


print(start_time,'\n')
print(end_time,'\n')
print(G_number)

from datetime import datetime, timedelta

def sub_time(input_datetime):

    result_datetime = input_datetime - timedelta(minutes=29)

    return result_datetime


def add_time(input_datetime):

    result_datetime = input_datetime + timedelta(minutes=29)

    return result_datetime

i=0
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg1_df1 = df1[(df1['DATE TIME'] >= start_date) & (df1['DATE TIME'] <= end_date)]
print(mfg1_df1)

i=1
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg1_df2 = df1[(df1['DATE TIME'] >= start_date) & (df1['DATE TIME'] <= end_date)]
print(mfg1_df2)

i=2
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg3_df3 = df3[(df3['DATE TIME'] >= start_date) & (df3['DATE TIME'] <= end_date)]
print(mfg3_df3)

i=3
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg6_df4 = df6[(df6['DATE TIME'] >= start_date) & (df6['DATE TIME'] <= end_date)]
print(mfg6_df4)

i=4
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg6_df5 = df6[(df6['DATE TIME'] >= start_date) & (df6['DATE TIME'] <= end_date)]
print(mfg6_df5)

i=5
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg7_df6 = df7[(df7['DATE TIME'] >= start_date) & (df7['DATE TIME'] <= end_date)]
print(mfg7_df6)

i=6
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg8_df7 = df8[(df8['DATE TIME'] >= start_date) & (df8['DATE TIME'] <= end_date)]
print(mfg8_df7)

#to be checked as the start date for the gasifier is in december 2020 and we have considered the tags data from jan 2021


i=7
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg9_df8 = df9[(df9['DATE TIME'] >= start_date) & (df9['DATE TIME'] <= end_date)]
print(mfg9_df8)





i=9
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg10_df10 = df10[(df10['DATE TIME'] >= start_date) & (df10['DATE TIME'] <= end_date)]
print(mfg10_df10)

i=8
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg9_df9 = df9[(df9['DATE TIME'] >= start_date) & (df9['DATE TIME'] <= end_date)]
print(mfg9_df9)

i=10
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
mfg10_df11 = df10[(df10['DATE TIME'] >= start_date) & (df10['DATE TIME'] <= end_date)]
print(mfg10_df11)

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


dfgd=pd.read_excel('GD_RUNS.xlsx')
dfgd

i=2
start=dfgd['Start Time'].tolist()
print(start[i])
end=dfgd['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
X = df2[(df2['DATE TIME'] >= start_date) & (df2['DATE TIME'] <= end_date)]
X.head()

data_columns = X.iloc[:,1:]

data_columns = data_columns.fillna(data_columns.mean())

dc_standardized = StandardScaler().fit_transform(data_columns)

n_components = 4
pca = PCA(n_components=n_components)
principal_components = pca.fit_transform(dc_standardized)

columns_pca = [f'PC{i+1}' for i in range(n_components)]
df_pca = pd.DataFrame(data=principal_components, columns=columns_pca)

import numpy as np

# Assuming X is your original data
# data_columns = X.iloc[:, 1:]
# data_columns = data_columns.fillna(data_columns.mean())

data_columns = X.iloc[:,1:]

data_columns = data_columns.fillna(data_columns.mean())

dc_standardized = StandardScaler().fit_transform(data_columns)

n_components = 4
pca = PCA(n_components=n_components)
principal_components = pca.fit_transform(dc_standardized)

# Columns for the PCA dataframe
columns_pca = [f'PC{i+1}' for i in range(n_components)]

# Create a DataFrame with the principal components
df_pca = pd.DataFrame(data=principal_components, columns=columns_pca)

# Get loadings for PC1 and PC2
loadings_pc1 = pca.components_[0]
loadings_pc2 = pca.components_[1]
loadings_pc3 = pca.components_[2]
loadings_pc4 = pca.components_[3]

# Normalize the loadings by dividing by the sum of absolute values
loadings_pc1_normalized = loadings_pc1 / np.sum(np.abs(loadings_pc1))
loadings_pc2_normalized = loadings_pc2 / np.sum(np.abs(loadings_pc2))
loadings_pc3_normalized = loadings_pc3 / np.sum(np.abs(loadings_pc3))
loadings_pc4_normalized = loadings_pc4 / np.sum(np.abs(loadings_pc4))

# Create a DataFrame to display the normalized loadings for PC1 and PC2
loadings_df = pd.DataFrame({
    'Variable': data_columns.columns,
    'PC1 Loading': loadings_pc1,
    'PC2 Loading': loadings_pc2,
    'PC3 Loading': loadings_pc3,
    'PC4 Loading': loadings_pc4
})

# Display the normalized loadings
print(loadings_df)

# Create a DataFrame to display the normalized loadings for PC1 and PC2
normalized_loadings_df = pd.DataFrame({
    'Variable': data_columns.columns,
    'PC1 NLoading': loadings_pc1_normalized,
    'PC2 NLoading': loadings_pc2_normalized,
    'PC3 NLoading': loadings_pc3_normalized,
    'PC4 NLoading': loadings_pc4_normalized
})

# Display the normalized loadings
print(normalized_loadings_df)

import matplotlib.pyplot as plt




plt.scatter(df_pca['PC1'], df_pca['PC2'])

# Identify 2 variables with highest absolute loadings for PC1 and PC2
top_variables_pc1 = normalized_loadings_df.sort_values(by='PC1 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc2 = normalized_loadings_df.sort_values(by='PC2 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc3 = normalized_loadings_df.sort_values(by='PC3 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()

# Highlight points associated with top variables in PC1 and PC2
for variable, row in zip(data_columns.columns, df_pca.iterrows()):
    index, values = row
    if variable in top_variables_pc1:
        plt.scatter(values['PC1'], values['PC2'], color='red', label=variable)
    elif variable in top_variables_pc2:
        plt.scatter(values['PC1'], values['PC2'], color='blue', label=variable)
    else:
        plt.scatter(values['PC1'], values['PC2'], color='green', alpha=0.5)

# Add legend
plt.legend()

# Show the plot
plt.show()








plt.scatter(df_pca['PC1'], df_pca['PC3'])

# Identify 2 variables with highest absolute loadings for PC1 and PC2
top_variables_pc1 = normalized_loadings_df.sort_values(by='PC1 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc2 = normalized_loadings_df.sort_values(by='PC2 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc3 = normalized_loadings_df.sort_values(by='PC3 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()

# Highlight points associated with top variables in PC1 and PC2
for variable, row in zip(data_columns.columns, df_pca.iterrows()):
    index, values = row
    if variable in top_variables_pc1:
        plt.scatter(values['PC1'], values['PC3'], color='red', label=variable)
    elif variable in top_variables_pc3:
        plt.scatter(values['PC1'], values['PC3'], color='blue', label=variable)
    else:
        plt.scatter(values['PC1'], values['PC3'], color='green', alpha=0.5)

# Add legend
plt.legend()

# Show the plot
plt.show()










plt.scatter(df_pca['PC2'], df_pca['PC3'])

# Identify 2 variables with highest absolute loadings for PC1 and PC2
top_variables_pc1 = normalized_loadings_df.sort_values(by='PC1 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc2 = normalized_loadings_df.sort_values(by='PC2 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc3 = normalized_loadings_df.sort_values(by='PC3 NLoading', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()


# Highlight points associated with top variables in PC1 and PC2
for variable, row in zip(data_columns.columns, df_pca.iterrows()):
    index, values = row
    if variable in top_variables_pc1:
        plt.scatter(values['PC2'], values['PC3'], color='red', label=variable)
    elif variable in top_variables_pc3:
        plt.scatter(values['PC2'], values['PC3'], color='blue', label=variable)
    else:
        plt.scatter(values['PC2'], values['PC3'], color='green', alpha=0.5)

# Add legend
plt.legend()

# Show the plot
plt.show()

# Combining the three lists into one list
all_variables = top_variables_pc1 + top_variables_pc2 + top_variables_pc3

# Creating a set to store unique values
unique_variables_set = set(all_variables)

# Converting the set back to a list if needed
unique_variables_list = list(unique_variables_set)

print(unique_variables_list)





import matplotlib.pyplot as plt

X['DATE TIME'] = pd.to_datetime(X['DATE TIME'])

# Plotting time graphs for each column
for column_name in unique_variables_list:
    if column_name != 'DATE TIME':  # Skip 'DATE TIME' column for individual plotting
        plt.figure(figsize=(10, 6))
        plt.plot(X['DATE TIME'], X[column_name], label=column_name)


        plt.xlabel('Date Time')
        plt.ylabel('Value')
        plt.title(f'Time Graph of {column_name}')

        # Adding legend
        plt.legend()

        # Show the plot
        plt.show()

import seaborn as sns

sns.heatmap(pca.components_, cmap='viridis', yticklabels=columns_pca, xticklabels=data_columns.columns)
plt.title('Principal Components loadings')
plt.show()