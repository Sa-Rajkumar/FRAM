# -*- coding: utf-8 -*-
"""MIXER_FAILURE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1euGtmmyU9vO-BnqHYGENJaj7A3sMMz60
"""

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

df1=pd.read_excel('Tags1.xlsx',sheet_name='G1')
df2=pd.read_excel('Tags1.xlsx',sheet_name='G2')
df3=pd.read_excel('Tags1.xlsx',sheet_name='G3')
df4=pd.read_excel('Tags1.xlsx',sheet_name='G4')
df5=pd.read_excel('Tags1.xlsx',sheet_name='G5')
df6=pd.read_excel('Tags1.xlsx',sheet_name='G6')
df7=pd.read_excel('Tags1.xlsx',sheet_name='G7')
df8=pd.read_excel('Tags1.xlsx',sheet_name='G8')
df9=pd.read_excel('Tags1.xlsx',sheet_name='G9')
df10=pd.read_excel('Tags1.xlsx',sheet_name='G10')

df=pd.read_excel('MIX_FAILURE.xlsx')
df

from datetime import datetime, timedelta

def sub_time(input_datetime):

    result_datetime = input_datetime - timedelta(minutes=29)

    return result_datetime


def add_time(input_datetime):

    result_datetime = input_datetime + timedelta(minutes=29)

    return result_datetime

i=4
start=df['Start Time'].tolist()
print(start[i])
end=df['End Time'].tolist()
print(end[i])


start_date = sub_time(start[i])
end_date = add_time(end[i])
X = df6[(df6['DATE TIME'] >= start_date) & (df6['DATE TIME'] <= end_date)]
X.head()

data_columns = X.iloc[:,1:]

data_columns = data_columns.fillna(data_columns.mean())

dc_standardized = StandardScaler().fit_transform(data_columns)

n_components = 3
pca = PCA(n_components=n_components)
principal_components = pca.fit_transform(dc_standardized)

columns_pca = [f'PC{i+1}' for i in range(n_components)]
df_pca = pd.DataFrame(data=principal_components, columns=columns_pca)
df_pca

import numpy as np

data_columns = X.iloc[:,1:]

data_columns = data_columns.fillna(data_columns.mean())

dc_standardized = StandardScaler().fit_transform(data_columns)

n_components = 3
pca = PCA(n_components=n_components)
principal_components = pca.fit_transform(dc_standardized)

# Columns for the PCA dataframe
columns_pca = [f'PC{i+1}' for i in range(n_components)]

# Create a DataFrame with the principal components
df_pca = pd.DataFrame(data=principal_components, columns=columns_pca)

# Get loadings for PC1 and PC2
loadings_pc1 = pca.components_[0]
loadings_pc2 = pca.components_[1]
loadings_pc3 = pca.components_[2]

loadings_df = pd.DataFrame({
    'Variable': data_columns.columns,
    'PC1 Loading': loadings_pc1,
    'PC2 Loading': loadings_pc2,
    'PC3 Loading': loadings_pc3
})

print(loadings_df)

# Normalize the loadings by dividing by the sum of absolute values
loadings_pc1_normalized = loadings_pc1 / np.sum(np.abs(loadings_pc1))
loadings_pc2_normalized = loadings_pc2 / np.sum(np.abs(loadings_pc2))
loadings_pc3_normalized = loadings_pc3 / np.sum(np.abs(loadings_pc3))

# Create a DataFrame to display the normalized loadings for PC1 and PC2
normalized_loadings_df = pd.DataFrame({
    'Variable': data_columns.columns,
    'PC1 Norm Load': loadings_pc1_normalized,
    'PC2 Norm Load': loadings_pc2_normalized,
    'PC3 Norm Load': loadings_pc3_normalized
})

print(normalized_loadings_df)

import matplotlib.pyplot as plt
plt.figure(figsize=(16, 4))
plt.scatter(df_pca['PC1'], df_pca['PC2'])

# Identify 2 variables with highest absolute loadings for PC1 and PC2
top_variables_pc1 = normalized_loadings_df.sort_values(by='PC1 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc2 = normalized_loadings_df.sort_values(by='PC2 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc3 = normalized_loadings_df.sort_values(by='PC3 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()

print(top_variables_pc1)
print(top_variables_pc2)
print(top_variables_pc3)

# Highlight points associated with top variables in PC1 and PC2
for variable, row in zip(data_columns.columns, df_pca.iterrows()):
    index, values = row
    if variable in top_variables_pc1:
        plt.scatter(values['PC1'], values['PC2'], color='red', label=variable)
    elif variable in top_variables_pc2:
        plt.scatter(values['PC1'], values['PC2'], color='blue', label=variable)
    else:
        plt.scatter(values['PC1'], values['PC2'], color='green', alpha=0.5)

# Add legend
plt.legend()

# Show the plot
plt.show()



import matplotlib.pyplot as plt
plt.figure(figsize=(16, 4))
plt.scatter(df_pca['PC1'], df_pca['PC3'])

# Identify 2 variables with highest absolute loadings for PC1 and PC2
top_variables_pc1 = normalized_loadings_df.sort_values(by='PC1 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc2 = normalized_loadings_df.sort_values(by='PC2 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc3 = normalized_loadings_df.sort_values(by='PC3 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()

# Highlight points associated with top variables in PC1 and PC2
for variable, row in zip(data_columns.columns, df_pca.iterrows()):
    index, values = row
    if variable in top_variables_pc1:
        plt.scatter(values['PC1'], values['PC3'], color='red', label=variable)
    elif variable in top_variables_pc3:
        plt.scatter(values['PC1'], values['PC3'], color='blue', label=variable)
    else:
        plt.scatter(values['PC1'], values['PC3'], color='green', alpha=0.5)

# Add legend
plt.legend()

# Show the plot
plt.show()





import matplotlib.pyplot as plt
plt.figure(figsize=(16, 4))
plt.scatter(df_pca['PC2'], df_pca['PC3'])

# Identify 2 variables with highest absolute loadings for PC1 and PC2
top_variables_pc1 = normalized_loadings_df.sort_values(by='PC1 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc2 = normalized_loadings_df.sort_values(by='PC2 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()
top_variables_pc3 = normalized_loadings_df.sort_values(by='PC3 Norm Load', key=lambda x: abs(x), ascending=False)['Variable'].head(2).tolist()


# Highlight points associated with top variables in PC1 and PC2
for variable, row in zip(data_columns.columns, df_pca.iterrows()):
    index, values = row
    if variable in top_variables_pc2:
        plt.scatter(values['PC2'], values['PC3'], color='red', label=variable)
    elif variable in top_variables_pc3:
        plt.scatter(values['PC2'], values['PC3'], color='blue', label=variable)
    else:
        plt.scatter(values['PC2'], values['PC3'], color='green', alpha=0.5)

# Add legend
plt.legend()

# Show the plot
plt.show()

# Combining the three lists into one list
all_variables = top_variables_pc1 + top_variables_pc2 + top_variables_pc3

# Creating a set to store unique values
unique_variables_set = set(all_variables)

# Convering the set back to a list
unique_variables_list = list(unique_variables_set)

print(unique_variables_list)


import matplotlib.pyplot as plt

X['DATE TIME'] = pd.to_datetime(X['DATE TIME'])

# Plotting time graphs for each column
for column_name in unique_variables_list:
    if column_name != 'DATE TIME':  # Skip 'DATE TIME' column for individual plotting
        plt.figure(figsize=(10, 4))
        plt.plot(X['DATE TIME'], X[column_name], label=column_name)


        plt.xlabel('Date Time')
        plt.ylabel('Value')
        plt.title(f'Time Graph of {column_name}')

        plt.legend()

        plt.show()

import seaborn as sns

sns.heatmap(pca.components_, cmap='viridis', yticklabels=columns_pca, xticklabels=data_columns.columns)
plt.title('Principal Components loadings')
plt.show()

plt.bar(range(1, n_components + 1), pca.explained_variance_ratio_)
plt.title('variance explained by each component')
plt.xlabel('principal component')
plt.ylabel('variance explained')
plt.show()